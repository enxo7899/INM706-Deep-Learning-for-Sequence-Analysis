{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPgPz3HH11n6JEVu9yWA+vK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enxo7899/INM706-Deep-Learning-for-Sequence-Analysis/blob/main/INM706_Seq2Seq_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacremoses\n",
        "pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIPBm9a6fjn1",
        "outputId": "9206b280-920d-48fc-c564-1441cc04e3d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/897.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m501.8/897.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.9/897.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qYG3KK9eT34",
        "outputId": "8043ff78-1682-483f-a49b-622a5075d521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentences sample:\n",
            "South Korea: North Korean Dictator, Kim Jong Il Is Dead · Global Voices\n",
            "Kim Jong Il, the North Korean dictator who ruled the hermit kingdom for the past three decades, has died at the age of 69.\n",
            "According to North Korean state television's official report on Monday, Kim passed away from \"mental and physical strain\" during a train ride on December 17, 2011.\n",
            "The South Korean Twittersphere erupted with various responses.\n",
            "Although the death of one of the world's most notorious dictators is something people might welcome, most South Koreans have expressed concern about the instability his sudden death might bring to Korean peninsula.\n",
            "\n",
            "Albanian sentences sample:\n",
            "Kore: Vdes diktatori koreano-verior, Kim Jong Il\n",
            "Kim Jong Il, diktatori koreano-verior, i cili sundoi me mbretërinë e izoluar gjatë tre dekadave të kaluara, vdiq në moshën 69 vjeçare.\n",
            "Sipas lajmit zyrtar të emituar ditën e hënë në televizionin shtetëror koreano-verior, Kim ka ndërruar jetë si rezultat i \"lodhjes mendore dhe fizike\" gjatë një udhëtimit me tren, më 17 dhjetor të vitit 2011.\n",
            "Twittersfera koreano-jugore shpërtheu me reagime të ndryshme.\n",
            "Edhe pse vdekja e njërit prej diktatorëve më famëkeq botërorë është diçka që njerëzit mund ta mirëpresin, shumica e koreano-jugorëve kanë shprehur shqetësimin e tyre për destabilizimin e mundshëm të gadishullit korean për shkak të vdekjes së tij të papritur.\n",
            "\n",
            "Total number of sentence pairs: 5784\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "with open('GlobalVoices.en-sq.en', 'r', encoding='utf-8') as f:\n",
        "    en_sentences = f.readlines()\n",
        "\n",
        "with open('GlobalVoices.en-sq.sq', 'r', encoding='utf-8') as f:\n",
        "    sq_sentences = f.readlines()\n",
        "\n",
        "# Print the first few lines of each dataset to understand the structure\n",
        "print(\"English sentences sample:\")\n",
        "for i in range(5):\n",
        "    print(en_sentences[i].strip())\n",
        "\n",
        "print(\"\\nAlbanian sentences sample:\")\n",
        "for i in range(5):\n",
        "    print(sq_sentences[i].strip())\n",
        "\n",
        "# Ensure both lists have the same length\n",
        "assert len(en_sentences) == len(sq_sentences),\n",
        "\n",
        "# Print the total number of sentences\n",
        "print(f\"\\nTotal number of sentence pairs: {len(en_sentences)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load the dataset\n",
        "with open('GlobalVoices.en-sq.en', 'r', encoding='utf-8') as f:\n",
        "    en_sentences = f.readlines()\n",
        "with open('GlobalVoices.en-sq.sq', 'r', encoding='utf-8') as f:\n",
        "    sq_sentences = f.readlines()\n",
        "\n",
        "# Verify dataset loaded correctly\n",
        "print(f\"English sentences sample: {en_sentences[:5]}\")\n",
        "print(f\"Albanian sentences sample: {sq_sentences[:5]}\")\n",
        "print(f\"Total number of sentence pairs: {len(en_sentences)}\")\n",
        "\n",
        "# Use MarianTokenizer for tokenization\n",
        "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-sq')\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, trg_sentences, tokenizer, max_length=128):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.trg_sentences = trg_sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_sentences[idx]\n",
        "        trg = self.trg_sentences[idx]\n",
        "\n",
        "        src_enc = self.tokenizer.encode_plus(\n",
        "            src,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        trg_enc = self.tokenizer.encode_plus(\n",
        "            trg,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'src': src_enc['input_ids'].squeeze(),\n",
        "            'src_mask': src_enc['attention_mask'].squeeze(),\n",
        "            'trg': trg_enc['input_ids'].squeeze(),\n",
        "            'trg_mask': trg_enc['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "# Create the dataset objects\n",
        "dataset = TranslationDataset(en_sentences, sq_sentences, tokenizer)\n",
        "\n",
        "# Split the dataset into train and validation sets (90% train, 10% validation)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "\n",
        "# Define the Seq2Seq model components\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        hidden = hidden.unsqueeze(0).repeat(2, 1, 1)\n",
        "        cell = cell[-2:].contiguous()\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = torch.sum(self.v * energy, dim=2)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM((hidden_dim * 2) + emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear((hidden_dim * 2) + hidden_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden[-1], encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n",
        "\n",
        "# Model hyperparameters\n",
        "INPUT_DIM = tokenizer.vocab_size\n",
        "OUTPUT_DIM = tokenizer.vocab_size\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# Initialize encoder, attention, decoder, and seq2seq model\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "attn = Attention(HID_DIM).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn).to(device)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "TRG_PAD_IDX = tokenizer.pad_token_id\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0  # Initialize epoch accuracy\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch['src'].T.to(device)\n",
        "        trg = batch['trg'].T.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = output.argmax(1)\n",
        "        non_pad_elements = (trg != TRG_PAD_IDX).nonzero().squeeze()\n",
        "        correct = preds[non_pad_elements].eq(trg[non_pad_elements]).sum().item()\n",
        "        acc = correct / len(non_pad_elements)\n",
        "        epoch_acc += acc\n",
        "\n",
        "        # Print some batches\n",
        "        if i % 10 == 0:\n",
        "            print(f'Batch {i} | Loss: {loss.item():.3f} | Accuracy: {acc:.3f}')\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch['src'].T.to(device)\n",
        "            trg = batch['trg'].T.to(device)\n",
        "            output = model(src, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = output.argmax(1)\n",
        "            non_pad_elements = (trg != TRG_PAD_IDX).nonzero().squeeze()\n",
        "            correct = preds[non_pad_elements].eq(trg[non_pad_elements]).sum().item()\n",
        "            acc = correct / len(non_pad_elements)\n",
        "            epoch_acc += acc\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train Acc: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} |  Val. Acc: {valid_acc:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92xRXGP_gPxC",
        "outputId": "e8ff32b9-9a54-4f38-ef2f-d972cee14910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "English sentences sample: ['South Korea: North Korean Dictator, Kim Jong Il Is Dead · Global Voices\\n', 'Kim Jong Il, the North Korean dictator who ruled the hermit kingdom for the past three decades, has died at the age of 69.\\n', 'According to North Korean state television\\'s official report on Monday, Kim passed away from \"mental and physical strain\" during a train ride on December 17, 2011.\\n', 'The South Korean Twittersphere erupted with various responses.\\n', \"Although the death of one of the world's most notorious dictators is something people might welcome, most South Koreans have expressed concern about the instability his sudden death might bring to Korean peninsula.\\n\"]\n",
            "Albanian sentences sample: ['Kore: Vdes diktatori koreano-verior, Kim Jong Il\\n', 'Kim Jong Il, diktatori koreano-verior, i cili sundoi me mbretërinë e izoluar gjatë tre dekadave të kaluara, vdiq në moshën 69 vjeçare.\\n', 'Sipas lajmit zyrtar të emituar ditën e hënë në televizionin shtetëror koreano-verior, Kim ka ndërruar jetë si rezultat i \"lodhjes mendore dhe fizike\" gjatë një udhëtimit me tren, më 17 dhjetor të vitit 2011.\\n', 'Twittersfera koreano-jugore shpërtheu me reagime të ndryshme.\\n', 'Edhe pse vdekja e njërit prej diktatorëve më famëkeq botërorë është diçka që njerëzit mund ta mirëpresin, shumica e koreano-jugorëve kanë shprehur shqetësimin e tyre për destabilizimin e mundshëm të gadishullit korean për shkak të vdekjes së tij të papritur.\\n']\n",
            "Total number of sentence pairs: 5784\n",
            "Data preprocessing complete.\n",
            "Batch 0 | Loss: 11.040 | Accuracy: 0.000\n",
            "Batch 10 | Loss: 6.802 | Accuracy: 0.027\n",
            "Batch 20 | Loss: 6.170 | Accuracy: 0.071\n",
            "Batch 30 | Loss: 6.065 | Accuracy: 0.082\n",
            "Batch 40 | Loss: 5.837 | Accuracy: 0.068\n",
            "Batch 50 | Loss: 5.617 | Accuracy: 0.094\n",
            "Batch 60 | Loss: 5.964 | Accuracy: 0.083\n",
            "Batch 70 | Loss: 5.528 | Accuracy: 0.096\n",
            "Batch 80 | Loss: 5.568 | Accuracy: 0.098\n",
            "Batch 90 | Loss: 5.934 | Accuracy: 0.095\n",
            "Batch 100 | Loss: 5.695 | Accuracy: 0.096\n",
            "Batch 110 | Loss: 5.765 | Accuracy: 0.088\n",
            "Batch 120 | Loss: 5.654 | Accuracy: 0.099\n",
            "Batch 130 | Loss: 5.606 | Accuracy: 0.113\n",
            "Batch 140 | Loss: 5.653 | Accuracy: 0.098\n",
            "Batch 150 | Loss: 5.461 | Accuracy: 0.116\n",
            "Batch 160 | Loss: 5.665 | Accuracy: 0.106\n",
            "Epoch: 01 | Time: 9m 47s\n",
            "\tTrain Loss: 5.904 | Train PPL: 366.394 | Train Acc: 0.090\n",
            "\t Val. Loss: 6.018 |  Val. PPL: 410.746 |  Val. Acc: 0.040\n",
            "Batch 0 | Loss: 5.529 | Accuracy: 0.097\n",
            "Batch 10 | Loss: 5.490 | Accuracy: 0.117\n",
            "Batch 20 | Loss: 5.409 | Accuracy: 0.104\n",
            "Batch 30 | Loss: 5.379 | Accuracy: 0.109\n",
            "Batch 40 | Loss: 5.370 | Accuracy: 0.130\n",
            "Batch 50 | Loss: 5.454 | Accuracy: 0.121\n",
            "Batch 60 | Loss: 5.360 | Accuracy: 0.110\n",
            "Batch 70 | Loss: 5.386 | Accuracy: 0.111\n",
            "Batch 80 | Loss: 5.427 | Accuracy: 0.106\n",
            "Batch 90 | Loss: 5.321 | Accuracy: 0.122\n",
            "Batch 100 | Loss: 5.255 | Accuracy: 0.131\n",
            "Batch 110 | Loss: 5.368 | Accuracy: 0.118\n",
            "Batch 120 | Loss: 5.408 | Accuracy: 0.122\n",
            "Batch 130 | Loss: 5.333 | Accuracy: 0.119\n",
            "Batch 140 | Loss: 5.245 | Accuracy: 0.139\n",
            "Batch 150 | Loss: 5.424 | Accuracy: 0.131\n",
            "Batch 160 | Loss: 5.445 | Accuracy: 0.123\n",
            "Epoch: 02 | Time: 9m 47s\n",
            "\tTrain Loss: 5.394 | Train PPL: 220.082 | Train Acc: 0.119\n",
            "\t Val. Loss: 5.898 |  Val. PPL: 364.175 |  Val. Acc: 0.073\n",
            "Batch 0 | Loss: 5.414 | Accuracy: 0.105\n",
            "Batch 10 | Loss: 5.281 | Accuracy: 0.125\n",
            "Batch 20 | Loss: 5.276 | Accuracy: 0.128\n",
            "Batch 30 | Loss: 5.551 | Accuracy: 0.126\n",
            "Batch 40 | Loss: 5.329 | Accuracy: 0.124\n",
            "Batch 50 | Loss: 5.287 | Accuracy: 0.132\n",
            "Batch 60 | Loss: 5.410 | Accuracy: 0.117\n",
            "Batch 70 | Loss: 5.326 | Accuracy: 0.124\n",
            "Batch 80 | Loss: 5.223 | Accuracy: 0.132\n",
            "Batch 90 | Loss: 5.034 | Accuracy: 0.137\n",
            "Batch 100 | Loss: 5.182 | Accuracy: 0.146\n",
            "Batch 110 | Loss: 5.251 | Accuracy: 0.119\n",
            "Batch 120 | Loss: 5.339 | Accuracy: 0.114\n",
            "Batch 130 | Loss: 5.173 | Accuracy: 0.125\n",
            "Batch 140 | Loss: 4.945 | Accuracy: 0.147\n",
            "Batch 150 | Loss: 5.151 | Accuracy: 0.146\n",
            "Batch 160 | Loss: 5.364 | Accuracy: 0.123\n",
            "Epoch: 03 | Time: 9m 47s\n",
            "\tTrain Loss: 5.234 | Train PPL: 187.604 | Train Acc: 0.130\n",
            "\t Val. Loss: 5.819 |  Val. PPL: 336.675 |  Val. Acc: 0.075\n",
            "Batch 0 | Loss: 5.216 | Accuracy: 0.143\n",
            "Batch 10 | Loss: 5.184 | Accuracy: 0.118\n",
            "Batch 20 | Loss: 5.043 | Accuracy: 0.151\n",
            "Batch 30 | Loss: 5.268 | Accuracy: 0.116\n",
            "Batch 40 | Loss: 5.145 | Accuracy: 0.127\n",
            "Batch 50 | Loss: 5.148 | Accuracy: 0.146\n",
            "Batch 60 | Loss: 5.115 | Accuracy: 0.134\n",
            "Batch 70 | Loss: 5.138 | Accuracy: 0.130\n",
            "Batch 80 | Loss: 5.297 | Accuracy: 0.138\n",
            "Batch 90 | Loss: 5.093 | Accuracy: 0.145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# Set the notebook name\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"INM706-Seq2Seq_Machine_Translation.ipynb\"\n",
        "\n",
        "# Login with the API KEY\n",
        "wandb.login(key=\"9ce954fd827fd8d839648cb3708ff788ad51bafa\")\n",
        "\n",
        "# Initialize wandb run\n",
        "wandb.init(project='Translator', name='English-Albanian')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load the dataset\n",
        "with open('GlobalVoices.en-sq.en', 'r', encoding='utf-8') as f:\n",
        "    en_sentences = f.readlines()\n",
        "with open('GlobalVoices.en-sq.sq', 'r', encoding='utf-8') as f:\n",
        "    sq_sentences = f.readlines()\n",
        "\n",
        "# Verify dataset loaded correctly\n",
        "print(f\"English sentences sample: {en_sentences[:5]}\")\n",
        "print(f\"Albanian sentences sample: {sq_sentences[:5]}\")\n",
        "print(f\"Total number of sentence pairs: {len(en_sentences)}\")\n",
        "\n",
        "# Use MarianTokenizer for tokenization\n",
        "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-sq')\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, trg_sentences, tokenizer, max_length=128):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.trg_sentences = trg_sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_sentences[idx]\n",
        "        trg = self.trg_sentences[idx]\n",
        "\n",
        "        src_enc = self.tokenizer.encode_plus(\n",
        "            src,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        trg_enc = self.tokenizer.encode_plus(\n",
        "            trg,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'src': src_enc['input_ids'].squeeze(),\n",
        "            'src_mask': src_enc['attention_mask'].squeeze(),\n",
        "            'trg': trg_enc['input_ids'].squeeze(),\n",
        "            'trg_mask': trg_enc['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "# Create the dataset objects\n",
        "dataset = TranslationDataset(en_sentences, sq_sentences, tokenizer)\n",
        "\n",
        "# Split the dataset into train and validation sets (90% train, 10% validation)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "\n",
        "# Define the Seq2Seq model components\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        hidden = hidden.unsqueeze(0).repeat(2, 1, 1)\n",
        "        cell = cell[-2:].contiguous()\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = torch.sum(self.v * energy, dim=2)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM((hidden_dim * 2) + emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear((hidden_dim * 2) + hidden_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden[-1], encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n",
        "\n",
        "# Model hyperparameters\n",
        "INPUT_DIM = tokenizer.vocab_size\n",
        "OUTPUT_DIM = tokenizer.vocab_size\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# Initialize wandb configuration\n",
        "wandb.config.update({\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 32,\n",
        "    \"encoder_embedding_dim\": ENC_EMB_DIM,\n",
        "    \"decoder_embedding_dim\": DEC_EMB_DIM,\n",
        "    \"hidden_dim\": HID_DIM,\n",
        "    \"num_layers\": N_LAYERS,\n",
        "    \"encoder_dropout\": ENC_DROPOUT,\n",
        "    \"decoder_dropout\": DEC_DROPOUT\n",
        "})\n",
        "\n",
        "# Initialize encoder, attention, decoder, and seq2seq model\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "attn = Attention(HID_DIM).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn).to(device)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
        "TRG_PAD_IDX = tokenizer.pad_token_id\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch['src'].T.to(device)\n",
        "        trg = batch['trg'].T.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = output.argmax(1)\n",
        "        non_pad_elements = (trg != TRG_PAD_IDX).nonzero().squeeze()\n",
        "        correct = preds[non_pad_elements].eq(trg[non_pad_elements]).sum().item()\n",
        "        acc = correct / len(non_pad_elements)\n",
        "        epoch_acc += acc\n",
        "\n",
        "        # Log metrics to wandb\n",
        "        wandb.log({\"batch_loss\": loss.item(), \"batch_accuracy\": acc})\n",
        "\n",
        "        # Print some batches\n",
        "        if i % 10 == 0:\n",
        "            print(f'Batch {i} | Loss: {loss.item():.3f} | Accuracy: {acc:.3f}')\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch['src'].T.to(device)\n",
        "            trg = batch['trg'].T.to(device)\n",
        "            output = model(src, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = output.argmax(1)\n",
        "            non_pad_elements = (trg != TRG_PAD_IDX).nonzero().squeeze()\n",
        "            correct = preds[non_pad_elements].eq(trg[non_pad_elements]).sum().item()\n",
        "            acc = correct / len(non_pad_elements)\n",
        "            epoch_acc += acc\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "N_EPOCHS = wandb.config.epochs\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train Acc: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} |  Val. Acc: {valid_acc:.3f}')\n",
        "\n",
        "    # Log epoch metrics to wandb\n",
        "    wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_acc,\n",
        "               \"valid_loss\": valid_loss, \"valid_accuracy\": valid_acc,\n",
        "               \"epoch\": epoch + 1, \"epoch_time_mins\": epoch_mins, \"epoch_time_secs\": epoch_secs})\n"
      ],
      "metadata": {
        "id": "96o_pyEfnc9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}