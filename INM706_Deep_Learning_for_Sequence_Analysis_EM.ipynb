{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "Y1g1bnsRg1cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('QandA.csv')\n",
        "\n",
        "# Check the columns of the dataframe\n",
        "print(df.columns)\n",
        "\n",
        "# Columns are 'Question' and 'Answer'\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())\n",
        "\n",
        "# Convert all values to strings and handle missing values\n",
        "df['Question'] = df['Question'].astype(str).fillna('')\n",
        "df['Answer'] = df['Answer'].astype(str).fillna('')\n",
        "\n",
        "# Add special tokens to the answers\n",
        "df['Answer'] = df['Answer'].apply(lambda x: '<start> ' + x + ' <end>')\n",
        "\n",
        "# Preprocess the data\n",
        "questions = df['Question'].values\n",
        "answers = df['Answer'].values\n",
        "\n",
        "# Tokenize the questions and answers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(list(questions) + list(answers))\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "special_tokens = {'<start>': len(tokenizer.word_index) + 1, '<end>': len(tokenizer.word_index) + 2}\n",
        "tokenizer.word_index.update(special_tokens)\n",
        "tokenizer.index_word[special_tokens['<start>']] = '<start>'\n",
        "tokenizer.index_word[special_tokens['<end>']] = '<end>'\n",
        "\n",
        "# Convert text to sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_len = max(max(len(seq) for seq in question_sequences), max(len(seq) for seq in answer_sequences))\n",
        "question_sequences = pad_sequences(question_sequences, maxlen=max_len, padding='post')\n",
        "answer_sequences = pad_sequences(answer_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Create training and validation splits\n",
        "X_train, X_val, y_train, y_val = train_test_split(question_sequences, answer_sequences, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, questions, answers):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.answers[idx]\n",
        "\n",
        "train_dataset = QADataset(X_train, y_train)\n",
        "val_dataset = QADataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQkrgy6DgD__",
        "outputId": "a24ff74e-86b9-4d5f-8b1b-28f884394de7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Index(['Question', 'Answer'], dtype='object')\n",
            "                                     Question  \\\n",
            "0    How do I take a screenshot on an iPhone?   \n",
            "1  How do I change my wallpaper on an iPhone?   \n",
            "2    How do I make a phone call on an iPhone?   \n",
            "3  How do I send a text message on an iPhone?   \n",
            "4             How do I use Siri on an iPhone?   \n",
            "\n",
            "                                              Answer  \n",
            "0  To take a screenshot on an iPhone, press and h...  \n",
            "1  To change your wallpaper on an iPhone, go to S...  \n",
            "2  To make a phone call on an iPhone, open the Ph...  \n",
            "3  To send a text message on an iPhone, open the ...  \n",
            "4  To use Siri on an iPhone, press and hold the H...  \n",
            "Data preprocessing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Seq2Seq Model with Attention"
      ],
      "metadata": {
        "id": "Qo_INCWVhYKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism that allows the decoder to focus on different parts of the encoder's outputs.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): The number of features in the hidden state of the LSTM.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        # Linear layer to calculate attention energies\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        # Vector used to calculate the weighted sum of attention energies\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Calculate the attention weights.\n",
        "\n",
        "        Args:\n",
        "            hidden (torch.Tensor): The decoder's current hidden state.\n",
        "            encoder_outputs (torch.Tensor): The encoder's outputs.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The attention weights.\n",
        "        \"\"\"\n",
        "        # Get the length of the input sequence\n",
        "        timestep = encoder_outputs.size(1)\n",
        "        # Repeat the hidden state across the input sequence length\n",
        "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
        "        # Calculate the attention energies\n",
        "        attn_energies = self.score(h, encoder_outputs)\n",
        "        # Return softmax-normalized attention weights\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "    def score(self, hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Calculate the attention scores.\n",
        "\n",
        "        Args:\n",
        "            hidden (torch.Tensor): The decoder's current hidden state.\n",
        "            encoder_outputs (torch.Tensor): The encoder's outputs.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The attention scores.\n",
        "        \"\"\"\n",
        "        # Concatenate hidden state and encoder outputs, and pass through a tanh-activated linear layer\n",
        "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
        "        # Transpose to match dimensions for batch matrix multiplication\n",
        "        energy = energy.transpose(2, 1)\n",
        "        # Repeat the attention vector across the batch\n",
        "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
        "        # Calculate the attention scores\n",
        "        energy = torch.bmm(v, energy)\n",
        "        return energy.squeeze(1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder that processes the input sequence and outputs hidden states.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        embed_size (int): Size of the embedding vectors.\n",
        "        hidden_size (int): The number of features in the hidden state of the LSTM.\n",
        "        num_layers (int): Number of recurrent layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Embedding layer to convert input words to embedding vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the encoder.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input sequence.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Outputs, hidden state, and cell state of the LSTM.\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(x)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder that generates the output sequence using the encoder's hidden states and attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        embed_size (int): Size of the embedding vectors.\n",
        "        hidden_size (int): The number of features in the hidden state of the LSTM.\n",
        "        num_layers (int): Number of recurrent layers.\n",
        "        attention (Attention): The attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, attention):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Embedding layer to convert input words to embedding vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # LSTM layer with input size increased by hidden_size to accommodate attention context vector\n",
        "        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        # Fully connected layer to generate predictions\n",
        "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
        "        # Attention mechanism\n",
        "        self.attention = attention\n",
        "\n",
        "    def forward(self, x, hidden, cell, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the decoder.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input word.\n",
        "            hidden (torch.Tensor): The decoder's current hidden state.\n",
        "            cell (torch.Tensor): The decoder's current cell state.\n",
        "            encoder_outputs (torch.Tensor): The encoder's outputs.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Predictions, hidden state, cell state, and attention weights.\n",
        "        \"\"\"\n",
        "        x = x.unsqueeze(1)  # Add batch dimension\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Calculate attention weights and context vector\n",
        "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs)\n",
        "\n",
        "        # Concatenate embedding and context vector\n",
        "        rnn_input = torch.cat([embedded, context], 2)\n",
        "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "\n",
        "        output = output.squeeze(1)  # Remove batch dimension\n",
        "        context = context.squeeze(1)  # Remove batch dimension\n",
        "        output = self.fc(torch.cat([output, context], 1))  # Generate predictions\n",
        "\n",
        "        return output, hidden, cell, attn_weights\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Seq2Seq model that combines the encoder and decoder with an attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        encoder (Encoder): The encoder module.\n",
        "        decoder (Decoder): The decoder module.\n",
        "        target_vocab_size (int): Size of the target vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, target_vocab_size):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass through the Seq2Seq model.\n",
        "\n",
        "        Args:\n",
        "            source (torch.Tensor): The source sequence.\n",
        "            target (torch.Tensor): The target sequence.\n",
        "            teacher_forcing_ratio (float): Probability of using teacher forcing.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The generated outputs.\n",
        "        \"\"\"\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "\n",
        "        # Tensor to store the decoder outputs\n",
        "        outputs = torch.zeros(batch_size, target_len, self.target_vocab_size).to(target.device)\n",
        "\n",
        "        # Pass the source sequence through the encoder\n",
        "        encoder_outputs, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # First input to the decoder is the <start> token\n",
        "        x = target[:, 0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Pass the previous output and hidden states through the decoder\n",
        "            output, hidden, cell, _ = self.decoder(x, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            x = target[:, t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "i5Bkw4FwrRhp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        timestep = encoder_outputs.size(1)\n",
        "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
        "        attn_energies = self.score(h, encoder_outputs)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "    def score(self, hidden, encoder_outputs):\n",
        "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
        "        energy = energy.transpose(2, 1)\n",
        "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
        "        energy = torch.bmm(v, energy)\n",
        "        return energy.squeeze(1)\n"
      ],
      "metadata": {
        "id": "FDPDP7mOgHqx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(x)\n",
        "        return outputs, hidden, cell\n"
      ],
      "metadata": {
        "id": "LBxN6dH0hcyz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, attention):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
        "        self.attention = attention\n",
        "\n",
        "    def forward(self, x, hidden, cell, encoder_outputs):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat([embedded, context], 2)\n",
        "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "\n",
        "        output = output.squeeze(1)\n",
        "        context = context.squeeze(1)\n",
        "        output = self.fc(torch.cat([output, context], 1))\n",
        "\n",
        "        return output, hidden, cell, attn_weights\n"
      ],
      "metadata": {
        "id": "hRkypkjDhfED"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, target_vocab_size):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, self.target_vocab_size).to(target.device)\n",
        "\n",
        "        encoder_outputs, hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[:, 0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell, _ = self.decoder(x, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            x = target[:, t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "lfilwjCZhhbz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "xHI3KbEqhmJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 20\n",
        "target_vocab_size = vocab_size\n",
        "\n",
        "# Initialize the encoder, decoder, attention, and Seq2Seq model\n",
        "attention = Attention(hidden_size)\n",
        "encoder = Encoder(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "decoder = Decoder(vocab_size, embed_size, hidden_size, num_layers, attention).to(device)\n",
        "model = Seq2Seq(encoder, decoder, target_vocab_size).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def accuracy_metric(pred, target, pad_idx):\n",
        "    pred_flat = pred.argmax(1)\n",
        "    non_pad_elements = (target != pad_idx).nonzero()\n",
        "    correct = (pred_flat[non_pad_elements] == target[non_pad_elements]).sum().item()\n",
        "    total = non_pad_elements.shape[0]\n",
        "    return correct / total\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'seq2seq-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYjz4GOehkVe",
        "outputId": "e7965a61-f879-443f-e5a4-ea0e6bdfce1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 6.093\n",
            "\t Val. Loss: 5.744\n",
            "Epoch: 02\n",
            "\tTrain Loss: 5.520\n",
            "\t Val. Loss: 5.697\n",
            "Epoch: 03\n",
            "\tTrain Loss: 5.424\n",
            "\t Val. Loss: 5.652\n",
            "Epoch: 04\n",
            "\tTrain Loss: 5.372\n",
            "\t Val. Loss: 5.633\n",
            "Epoch: 05\n",
            "\tTrain Loss: 5.333\n",
            "\t Val. Loss: 5.624\n",
            "Epoch: 06\n",
            "\tTrain Loss: 5.298\n",
            "\t Val. Loss: 5.617\n",
            "Epoch: 07\n",
            "\tTrain Loss: 5.263\n",
            "\t Val. Loss: 5.622\n",
            "Epoch: 08\n",
            "\tTrain Loss: 5.233\n",
            "\t Val. Loss: 5.595\n",
            "Epoch: 09\n",
            "\tTrain Loss: 5.219\n",
            "\t Val. Loss: 5.593\n",
            "Epoch: 10\n",
            "\tTrain Loss: 5.185\n",
            "\t Val. Loss: 5.614\n",
            "Epoch: 11\n",
            "\tTrain Loss: 5.169\n",
            "\t Val. Loss: 5.600\n",
            "Epoch: 12\n",
            "\tTrain Loss: 5.149\n",
            "\t Val. Loss: 5.593\n",
            "Epoch: 13\n",
            "\tTrain Loss: 5.131\n",
            "\t Val. Loss: 5.585\n",
            "Epoch: 14\n",
            "\tTrain Loss: 5.114\n",
            "\t Val. Loss: 5.601\n",
            "Epoch: 15\n",
            "\tTrain Loss: 5.089\n",
            "\t Val. Loss: 5.604\n",
            "Epoch: 16\n",
            "\tTrain Loss: 5.066\n",
            "\t Val. Loss: 5.586\n",
            "Epoch: 17\n",
            "\tTrain Loss: 5.051\n",
            "\t Val. Loss: 5.589\n",
            "Epoch: 18\n",
            "\tTrain Loss: 5.039\n",
            "\t Val. Loss: 5.595\n",
            "Epoch: 19\n",
            "\tTrain Loss: 5.011\n",
            "\t Val. Loss: 5.594\n",
            "Epoch: 20\n",
            "\tTrain Loss: 4.998\n",
            "\t Val. Loss: 5.597\n",
            "Epoch: 21\n",
            "\tTrain Loss: 4.988\n",
            "\t Val. Loss: 5.586\n",
            "Epoch: 22\n",
            "\tTrain Loss: 4.969\n",
            "\t Val. Loss: 5.579\n",
            "Epoch: 23\n",
            "\tTrain Loss: 4.959\n",
            "\t Val. Loss: 5.599\n",
            "Epoch: 24\n",
            "\tTrain Loss: 4.945\n",
            "\t Val. Loss: 5.585\n",
            "Epoch: 25\n",
            "\tTrain Loss: 4.936\n",
            "\t Val. Loss: 5.585\n",
            "Epoch: 26\n",
            "\tTrain Loss: 4.921\n",
            "\t Val. Loss: 5.579\n",
            "Epoch: 27\n",
            "\tTrain Loss: 4.903\n",
            "\t Val. Loss: 5.581\n",
            "Epoch: 28\n",
            "\tTrain Loss: 4.892\n",
            "\t Val. Loss: 5.584\n",
            "Epoch: 29\n",
            "\tTrain Loss: 4.875\n",
            "\t Val. Loss: 5.595\n",
            "Epoch: 30\n",
            "\tTrain Loss: 4.872\n",
            "\t Val. Loss: 5.598\n",
            "Epoch: 31\n",
            "\tTrain Loss: 4.856\n",
            "\t Val. Loss: 5.598\n",
            "Epoch: 32\n",
            "\tTrain Loss: 4.845\n",
            "\t Val. Loss: 5.601\n",
            "Epoch: 33\n",
            "\tTrain Loss: 4.828\n",
            "\t Val. Loss: 5.598\n",
            "Epoch: 34\n",
            "\tTrain Loss: 4.812\n",
            "\t Val. Loss: 5.606\n",
            "Epoch: 35\n",
            "\tTrain Loss: 4.802\n",
            "\t Val. Loss: 5.595\n",
            "Epoch: 36\n",
            "\tTrain Loss: 4.799\n",
            "\t Val. Loss: 5.601\n",
            "Epoch: 37\n",
            "\tTrain Loss: 4.788\n",
            "\t Val. Loss: 5.610\n",
            "Epoch: 38\n",
            "\tTrain Loss: 4.804\n",
            "\t Val. Loss: 5.607\n",
            "Epoch: 39\n",
            "\tTrain Loss: 4.779\n",
            "\t Val. Loss: 5.586\n",
            "Epoch: 40\n",
            "\tTrain Loss: 4.775\n",
            "\t Val. Loss: 5.603\n",
            "Epoch: 41\n",
            "\tTrain Loss: 4.757\n",
            "\t Val. Loss: 5.585\n",
            "Epoch: 42\n",
            "\tTrain Loss: 4.744\n",
            "\t Val. Loss: 5.596\n",
            "Epoch: 43\n",
            "\tTrain Loss: 4.743\n",
            "\t Val. Loss: 5.605\n",
            "Epoch: 44\n",
            "\tTrain Loss: 4.741\n",
            "\t Val. Loss: 5.596\n",
            "Epoch: 45\n",
            "\tTrain Loss: 4.738\n",
            "\t Val. Loss: 5.609\n",
            "Epoch: 46\n",
            "\tTrain Loss: 4.714\n",
            "\t Val. Loss: 5.596\n",
            "Epoch: 47\n",
            "\tTrain Loss: 4.708\n",
            "\t Val. Loss: 5.620\n",
            "Epoch: 48\n",
            "\tTrain Loss: 4.706\n",
            "\t Val. Loss: 5.600\n",
            "Epoch: 49\n",
            "\tTrain Loss: 4.692\n",
            "\t Val. Loss: 5.607\n",
            "Epoch: 50\n",
            "\tTrain Loss: 4.693\n",
            "\t Val. Loss: 5.616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question, model, tokenizer, max_len, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize and pad the input question\n",
        "        question_seq = tokenizer.texts_to_sequences([question])\n",
        "        question_seq = pad_sequences(question_seq, maxlen=max_len, padding='post')\n",
        "        question_seq = torch.tensor(question_seq, dtype=torch.long).to(device)\n",
        "\n",
        "        # Initialize hidden state and cell state\n",
        "        encoder_outputs, hidden, cell = model.encoder(question_seq)\n",
        "\n",
        "        # Prepare the first input for the decoder\n",
        "        start_token_index = tokenizer.word_index.get('<start>', 1)  # default to 1 if not found\n",
        "        x = torch.tensor([start_token_index], dtype=torch.long).to(device)\n",
        "\n",
        "        # Collect the generated tokens\n",
        "        generated_tokens = []\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell, _ = model.decoder(x, hidden, cell, encoder_outputs)\n",
        "            pred_token = output.argmax(1).item()\n",
        "            generated_tokens.append(pred_token)\n",
        "            x = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
        "            if pred_token == tokenizer.word_index.get('<end>', 2):  # default to 2 if not found\n",
        "                break\n",
        "\n",
        "        # Convert tokens back to words\n",
        "        response = tokenizer.sequences_to_texts([generated_tokens])\n",
        "        return response[0]\n",
        "\n",
        "# Example usage\n",
        "question = \"How do I use the built-in Camera app to take photos and videos?\"\n",
        "response = generate_response(question, model, tokenizer, max_len, device)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m2b73X-jijg",
        "outputId": "45820798-1f6b-4a0b-8ca0-9d96a08aa347"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes to can use app on and and and the the and the end end end end end end end end end the the the a and and and the the the the and and the the the the the the the to and then the the end end end end on on your your your to to your your end end end end end end the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        }
      ]
    }
  ]
}